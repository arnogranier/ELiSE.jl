{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions;\n",
    "using Images;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "As in the preprint Supp Table 1. A few from the experimental setup are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const dt::Float64 = 0.1\n",
    "const N_trials::Int = 10000\n",
    "const T::Int = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const N_out::Int = 13\n",
    "const N_latent::Int = 30\n",
    "const p0::Float64 = 0.04\n",
    "const p::Float64 = 0.2\n",
    "const q::Float64 = 0.15\n",
    "\n",
    "const N::Int = N_out + N_latent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "const E1::Float64 = -70\n",
    "const E_exc::Float64 = 0\n",
    "const E_inh::Float64 = -75\n",
    "const gl::Float64 = 0.1\n",
    "const g_den::Float64 = 2.0\n",
    "const g_exc_0::Float64 = 0.3\n",
    "const g_inh_0::Float64 = 6.0\n",
    "const a::Float64 = 0.3\n",
    "const b::Float64 = -58.\n",
    "d_den_init = Uniform(5., 15.)\n",
    "d_som_init = Uniform(5., 15.)\n",
    "const delta::Float64 = 25.\n",
    "\n",
    "rho(u) = 1. / (1. + exp(a*(b-u)))\n",
    "\n",
    "d_den::Vector{Float64} = round.(rand(d_den_init, N), digits=1)\n",
    "d_som_exc::Vector{Float64} = round.(rand(d_som_init, N), digits=1)\n",
    "d_som_inh::Vector{Float64} = d_som_exc .+ delta;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const η0::Float64 = 0.0001\n",
    "const ηl::Float64 = 0.001\n",
    "const λ::Float64 = 0.6\n",
    "W_init = Normal(0., 0.5)\n",
    "\n",
    "lrs::Matrix{Float64} = mapreduce(permutedims, vcat, [[(i<=N_out && j<=N_out) ? η0 : ηl for j in 1:N] for i in 1:N]); #ugly, but necessary for not checking if i<N_out etc in step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you did this nice analysis, might as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(k) = (k == 0) ? p0 : (1-p0) * p^((k^2-k)/2) * (1-p^k)\n",
    "D_nout = DiscreteNonParametric(0:N_latent, P.(0:N_latent));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somato-somatic scafold\n",
    "\n",
    "Just an implementation of what is described in the paper. I did check if $d_{out}$ follow the correct distribution. Could certainly be better but this is by no mean a bottleneck. I just have the feeling that it might be overly complex for no practical reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "B::Matrix{Float64} = zeros(N_latent, N)\n",
    "fatigued = []\n",
    "nudged = vcat(1:N_out)\n",
    "latent = vcat(N_out+1:N)\n",
    "previously_accepted = Dict(i=>0 for i in latent)\n",
    "while !isempty(nudged)\n",
    "    i = rand(nudged)\n",
    "    X = rand(D_nout)\n",
    "    for c in 1:X\n",
    "        j = rand(latent)\n",
    "        while rand() > q^previously_accepted[j] || j==i\n",
    "            j = rand(latent)\n",
    "        end\n",
    "        previously_accepted[j] += 1\n",
    "        (j in fatigued || j in nudged) || push!(nudged, j)\n",
    "        B[j-N_out,i] = 1\n",
    "    end\n",
    "    deleteat!(nudged, findall(nudged.==i))\n",
    "    push!(fatigued, i)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Here I took a shitty approach in term of memory, but it makes things easier in terms of later indexing. I use 3 `Vector{Queue{Float64}}`, one for each delay type (dendritic, somatic excitatory, somatic inhibitory). Within a `Vector{Queue{Float64}}` for one type of delay, I start by filling in all the queues with $d_i/\\texttt{dt}$ initial values. Then at every step I use `pop!` once on every queue to get my delayed values, and at the end I `push!` once to every queue the current $r_i$ to the queue. In that sense, I do not need to index an history to get the delayed values, rather the length of each queue acts as the delay. Indexing a complete history is probably better, I just thought this was a funny implementation, and it shouldn't be much slower on CPU to be honest. Bellow are the functions that create and add to the queues. The `pop!` are in the `step!` function bellow.\n",
    "\n",
    "_In fact these are implemented as `Vector{Vector{Float64}}` because Julia has no implementation of `Queue` that is better than `push!(::Vector{Float64}, ::Float64)` and `popfirst!(::Vector{Float64})` but that's a detail._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_queues!(ds::Vector{Float64})::Vector{Vector{Float64}}\n",
    "    qs = [Vector{Float64}() for _ in 1:N]\n",
    "    initialize!((q,d)) = (x->push!(q,x)).(rho.(E1*ones(Int(d ÷ dt))))\n",
    "    initialize!.(zip(qs, ds))\n",
    "    return qs\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function memory!(r::Vector{Float64})\n",
    "    for (q_den, q_exc, q_inh, x) in zip(m_den, m_som_exc, m_som_inh, r)\n",
    "        push!(q_den, x)\n",
    "        push!(q_exc, x)\n",
    "        push!(q_inh, x)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step\n",
    "\n",
    "Quite straightforwardly implementing the equations. Here I did try to not be too dirty, as I don't want to wait for hours afterwards. \n",
    "I first do the (3) matrix multiplications that I found useful in implementing the equations. I then do the assignments in loops as it is the more performant way to do things in Julia on CPU. The 2 matrix multplications for somato-somatic synapses might be better as indexing...\n",
    "The structure is: \n",
    "```julia\n",
    "for output neurons\n",
    "    intrinsic u update\n",
    "    if teacher\n",
    "        teacher nudging\n",
    "    end\n",
    "end\n",
    "for latent neurons\n",
    "    u update\n",
    "end\n",
    "for all neurons # this could be done twice in the previous loops ... \n",
    "    v update    # ... but it's ugly\n",
    "    rbar update\n",
    "end\n",
    "update memory # it has been rolled already by the pop! in the u and v updates!\n",
    "for all somato-dendritic weights\n",
    "    plasticity\n",
    "end\n",
    "```\n",
    "This function `step!` is benchmarked at the end of this notebook, and on my hardware (cpu) takes around 4μs, so that's for dt ms of simulations. In other words, we are at approximatively 25x 'real'time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function step!(u::Vector{Float64},u_tgt::Vector{Float64},v::Vector{Float64},rbar::Vector{Float64},W::Matrix{Float64};teacher::Bool=true)\n",
    "    past_den_r = popfirst!.(m_den)\n",
    "    I_den = W * past_den_r\n",
    "    g_exc = g_exc_0 * B * max.(popfirst!.(m_som_exc), rho(E1))\n",
    "    g_inh = g_inh_0 * B * min.(popfirst!.(m_som_inh), rho(E1))\n",
    "    for i in 1:N_out\n",
    "        @inbounds u[i] += dt * (-gl*(u[i] .- E1) + g_den*(v[i]-u[i])) \n",
    "        if teacher \n",
    "            @inbounds u[i] += dt * (λ/(1-λ) * (gl+g_den) / (E_inh - E_exc)) * ((E_inh - u_tgt[i]) * (E_exc - u[i]) + (u_tgt[i]-E_exc) * (E_inh - u[i]))\n",
    "        end\n",
    "    end\n",
    "    for i in N_out+1:N\n",
    "        @inbounds u[i] += dt * (-gl*(u[i] .- E1) + g_den*(v[i]-u[i]) + g_exc[i-N_out] * (E_exc - u[i]) + g_inh[i-N_out] * (E_inh - u[i]))\n",
    "    end\n",
    "    for i in 1:N\n",
    "        @inbounds v[i] += dt * (-gl*(v[i] .- E1) + I_den[i])\n",
    "        @inbounds rbar[i] += dt * (-gl*rbar[i] .+ ((gl*g_den)/(gl+g_den)) * past_den_r[i])\n",
    "    end\n",
    "    r = rho.(u)\n",
    "    rstar = rho.((g_den/(gl+g_den))*v.+(gl*E1)/(gl+g_den))\n",
    "    memory!(r)\n",
    "    for i in 1:N, j in 1:N\n",
    "        @inbounds W[i,j] += dt*lrs[i,j]*(r[i]-rstar[i])*rbar[j]\n",
    "    end\n",
    "    return nothing;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(u::Vector{Float64}, v::Vector{Float64}, rbar::Vector{Float64}, W::Matrix{Float64}, target::Function, T::Int, n_trials::Int; test_every::Int=1000, θ::Float64=-60., dt::Float64=0.1)\n",
    "    for trial in 0:n_trials-1\n",
    "        for t in 0:dt:T-dt\n",
    "            step!(u, target(t), v, rbar, W, teacher=true)\n",
    "        end\n",
    "        (trial % test_every == 0) && test!(u, v, rbar, W, T, dt=dt, θ=θ)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and plotting\n",
    "\n",
    "I was quite proud of my idea of showing in black the times where the produced/replayed output agrees with the target and then in two different colors when only the target or the produced output was on. Then I remembered that Ben presented something like this at the Xmas symposium last year ahah. Anyway that's what I do for testing, much quantitative. See plot bellow produced by `train!` (in fact, by the calls of `test!` in `train!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test!(u::Vector{Float64}, v::Vector{Float64}, rbar::Vector{Float64}, W::Matrix{Float64}, T::Int; dt::Float64=0.1, θ::Float64=-60)\n",
    "    u_prod = Vector{Vector{Float64}}()\n",
    "    for t in 0:dt:T-dt\n",
    "        step!(u, zeros(N_out), v, rbar, W, teacher=false)\n",
    "        isinteger(t) && push!(u_prod, u[1:N_out])\n",
    "    end\n",
    "    check(vv) = reduce(hcat, vv) .> θ\n",
    "    display(colors[1 .+ 2*check(u_prod) + check([target(t) for t in 0:T-1])])\n",
    "end\n",
    "colors = [colorant\"white\", colorant\"red\", colorant\"cyan\", colorant\"black\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v::Vector{Float64} = E1*ones(N)\n",
    "u::Vector{Float64} = E1*ones(N)\n",
    "rbar::Vector{Float64} = rho.(u);\n",
    "W::Matrix{Float64} = rand(W_init, N, N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_den::Vector{Vector{Float64}} = create_queues!(d_den)\n",
    "m_som_exc::Vector{Vector{Float64}} = create_queues!(d_som_exc)\n",
    "m_som_inh::Vector{Vector{Float64}} = create_queues!(d_som_inh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAAnCAAAAACQpireAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAMBJREFUaAXtwYEJAzAMA8E3aP+V1Q2ESQkpRXciMsmQmGRIzLkhMcmQiFoTtSZzziRDYm4x50wiak3UmvjC8G+GRNSaqDUN58y5ITG3DIlJRK2JWpNJhltMMiTmnEmGRNSaqDURmTfMLxK1JmpNPDIk5g2TiFoTtabhnPk3QyJqTdSaTDIkQ2IS84tMImpN1JqGxNwynDO3DImoNVFrMrcMibllOGcSUWui1sQjQ2KS4Q1Ra6LWxDXmDXOLqDVRax8fpCGVCRI6VAAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAAnCAAAAACQpireAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAMBJREFUaAXtwYEJAzAMA8E3aP+V1Q2ESQkpRXciMsmQmGRIzLkhMcmQiFoTtSZzziRDYm4x50wiak3UmvjC8G+GRNSaqDUN58y5ITG3DIlJRK2JWpNJhltMMiTmnEmGRNSaqDURmTfMLxK1JmpNPDIk5g2TiFoTtabhnPk3QyJqTdSaTDIkQ2IS84tMImpN1JqGxNwynDO3DImoNVFrMrcMibllOGcSUWui1sQjQ2KS4Q1Ra6LWxDXmDXOLqDVRax8fpCGVCRI6VAAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "13×100 Matrix{Gray{Bool}}:\n",
       " 0  0  0  0  0  0  0  0  0  0  1  1  1  …  1  1  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  0  0  0     1  1  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 0  0  0  0  0  0  0  0  0  0  1  1  1     1  1  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     0  0  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chords = [rand(N_out).>.75 for j in 1:10]\n",
    "target(t) = E1*ones(N_out) + 20*Float64.(chords[Int(t÷10+1)]);\n",
    "broadcast.(Gray, (!).(mapreduce(permutedims,vcat,repeat(chords, inner=10))'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Float64.(rand(N_latent, N) .< 1/N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, red is target only, cyan is produced only, black is both, white is neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train!(u, v, rbar, W, target, T, N_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 8 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m3.628 μs\u001b[22m\u001b[39m … \u001b[35m 1.236 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 99.08%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m3.885 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m4.597 μs\u001b[22m\u001b[39m ± \u001b[32m15.864 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m4.82% ±  1.40%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m▅\u001b[39m█\u001b[39m▃\u001b[39m \u001b[39m \u001b[34m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▄\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▄\u001b[34m▃\u001b[39m\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[32m▁\u001b[39m\u001b[39m▂\u001b[39m▂\u001b[39m▄\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m \u001b[39m▃\n",
       "  3.63 μs\u001b[90m        Histogram: frequency by time\u001b[39m        5.89 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m3.33 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m18\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "u_tgt_bench::Vector{Float64} = E1*ones(N_out) + 20*Float64.(rand(N_out).<.5);\n",
    "@benchmark step!($u, $u_tgt_bench, $v, $rbar, $W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my way to simplification:\n",
    "\n",
    "$$v_{t+1} = \\gamma_v v_t + W_tr_{t-\\tau_d}$$\n",
    "$$u^l_{t+1} = \\gamma_u u^l_t + \\alpha_d v^l_t + \\alpha_e B r_{t-\\tau_e} \\odot (1-u^l_t) + \\alpha_i B r_{t-\\tau_e-\\delta} \\odot (-1-u^l_t)$$\n",
    "$$u^o_{t+1} = \\gamma_u u^o_t + \\alpha_d v^o_t - \\alpha_t(u^o_t-u_{tgt})$$\n",
    "$$\\bar{r}_{t+1} = \\gamma_r \\bar{r}_t + \\alpha_{r} r_{t-\\tau_d}$$\n",
    "$$W_{t+1} = W_t + \\eta\\left[r_t-\\rho(\\alpha_v v_t+k)\\right]\\bar{r}_t^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
